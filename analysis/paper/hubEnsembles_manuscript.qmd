---
title: "[hubEnsembles]{.pkg}: Ensembling Methods in [R]{.proglang}"
format:
  jss-pdf:
    keep-tex: true
    journal:
      type: article
      cite-shortnames: true
      suppress: [title]
      include-jss-default: false
author:
  - name: Li Shandross
    affiliations:
      - name: University of Massachusetts Amherst
    orcid: 0009-0008-1348-1954
    attributes:
      equal-contributor: true
    email: lshandross@umass.edu
  - name: Emily Howerton
    affiliations:
      - The Pennsylvania State University
    orcid: 0000-0002-0639-3728
    attributes:
      equal-contributor: true
  - name: Lucie Contamin
    affiliations:
      - University of Pittsburgh
    orcid: 0000-0001-5797-1279
  - name: Harry Hochheiser
    affiliations:
      - University of Pittsburgh
    orcid: 0000-0001-8793-9982
  - name: Anna Krystalli
    affiliations:
      - R-RSE SMPC
    orcid: 0000-0002-2378-4915
  - name: Consortium of Infectious Disease Modeling Hubs
    attributes:
      deceased: true # use as a proxy to flag consortium authorship for now
  - name: Nicholas G. Reich
    affiliations:
      - University of Massachusetts Amherst
    orcid: 0000-0003-3503-9899
  - name: Evan L. Ray
    affiliations:
      - University of Massachusetts Amherst
    orcid: 0000-0003-4035-0243
execute:
  echo: true
  warning: false
  message: false
bibliography: references.bib
abstract: |
  Combining predictions from multiple models into an ensemble is a widely used practice across many fields with demonstrated performance benefits. The [R]{.proglang} package [hubEnsembles]{.pkg} provides a flexible framework for  ensembling various types of predictions, including point estimates and probabilistic predictions. A range of common methods for generating ensembles are supported, including weighted averages, quantile averages, and linear pools. The [hubEnsembles]{.pkg} package fits within a broader framework of open-source software and data tools called the "hubverse", which facilitates the development and management of collaborative modelling exercises.
  
keywords: [multiple models, aggregation, forecast, prediction]
keywords-formatted: [multiple models, aggregation, forecast, prediction]
---

```{r setup}
#| echo: FALSE
library(hubEnsembles)
library(hubUtils)
library(hubVis)
library(dplyr)
library(tidyr)
library(lubridate)
library(patchwork)
library(ggplot2)
```


## Introduction {#sec-intro}

Predictions of future outcomes are essential to planning and decision making, yet generating reliable predictions of the future is challenging. One method for overcoming this challenge is combining predictions across multiple, independent models. These combination methods (also called aggregation or ensembling) have been repeatedly shown to produce predictions that are more accurate [@clemen1989; @timmermann2006b] and more consistent [@hibon2005] than individual models. Because of the clear performance benefits, multi-model ensembles are commonplace across fields, including weather [@alley2019], climate [@tebaldi2007], and economics [@aastveit2018]. More recently, multi-model ensembles have been used to improve predictions of infectious disease outbreaks [@viboud2018; @johansson2019; @mcgowan2019; @reich_accuracy_2019; @cramer2022].

In the rapidly growing field of outbreak forecasting, there are many proposed methods for generating ensembles. Generally, these methods differ in at least one of two ways: (1) the function used to combine or "average" predictions, and (2) how predictions are weighted when performing the combination. No one method is universally "the best"; a simple average of predictions works surprisingly well across a range of settings [@mcgowan2019; @paireau_ensemble_2022; @ray_comparing_2023] for established theoretical reasons [@winkler2015]. However, more complex approaches have also been shown to have benefits in some settings [@yamana_superensemble_2016; @ray_prediction_2018; @reich_accuracy_2019; @colon-gonzalez_probabilistic_2021]. Here, we present the [hubEnsembles]{.pkg} package, which provides a flexible framework for generating ensemble predictions from multiple models. Complementing other software for combining predictions from multiple models (e.g., [@pedregosa_scikit-learn_2011; @weiss2019; @bosse_stackr_2023; @couch_stacks_2023]), [hubEnsembles]{.pkg} supports multiple types of predictions, including point estimates and different kinds of probabilistic predictions. Throughout, we will use the term "prediction" to refer to any kind of model output that may be combined including a forecast, a scenario projection, or a parameter estimate.

The [hubEnsembles]{.pkg} package is part of the "hubverse" collection of open-source software and data tools. The hubverse project facilitates the development and management of collaborative modelling exercises [@hubverse_docs]. The broader hubverse initiative is motivated by the demonstrated benefits of collaborative hubs [@reich2022; @borchering_public_2023], including performance benefits of multi-model ensembles and the desire for standardization across such hubs. In this paper, we focus specifically on the functionality encompassed in [hubEnsembles]{.pkg}. We provide an overview of the methods implemented, including mathematical definitions and properties (@sec-defs) as well as implementation details (@sec-implementation); we give simple examples to demonstrate the functionality (@sec-simple-ex) and a more complex case study (@sec-flu) that motivates a discussion and comparison of the various methods (@sec-conclusions).

## Mathematical definitions and properties of ensemble methods {#sec-defs}

The [hubEnsembles]{.pkg} package supports both point predictions and probabilistic predictions of different formats. A point prediction gives a single estimate of a future outcome while a probabilistic prediction provides an estimated probability distribution over a set of future outcomes. We use $N$ to denote the total number of individual predictions that the ensemble will combine. For example, these predictions will often be produced by different statistical or mathematical models, and $N$ is the total number of models that have provided predictions. Individual predictions will be indexed by the subscript $i$. Optionally, the package allows for calculating ensembles that use a weight $w_i$ for each prediction; we define the set of model-specific weights as $\pmb{w} = \{w_i | i \in 1, ..., N\}$. Informally, predictions with a larger weight have a greater influence on the ensemble prediction, though the details of this depend on the ensemble method (described further below).

For a set of $N$ point predictions, $\pmb{p} = \{p_i|i \in 1, ..., N\}$, each from a distinct model $i$, the [hubEnsembles]{.pkg} package can compute an ensemble of these predictions

$$
p_E = C(\pmb{p}, \pmb{w}) 
$$

using any function $C$ and a any set of model-specific weights $\pmb{w}$. For example, an arithmetic average of predictions yields $p_E = \sum_{i=1}^Np_iw_i$, where the weights are non-negative and sum to 1. If $w_i = 1/N$ for all $i$, all predictions will be equally weighted. This framework can also support more complex functions for aggregation, such as a (weighted) median or geometric mean.

For probabilistic predictions, there are two commonly used classes of methods to average or ensemble multiple predictions: quantile averaging (also called a Vincent average [@vincent1912]) and probability averaging (also called a distributional mixture or linear opinion pool [@stone1961]) [@lichtendahl2013]. To define these two classes of methods, let $F(x)$ be a cumulative density function (CDF) defined over values $x$ of the target variable for the prediction, and $F^{-1}(\theta)$ be the corresponding quantile function defined over quantile levels $\theta \in [0, 1]$. Throughout this article, we may refer to $x$ as either a 'value of the target variable' or a 'quantile' depending on the context, and similarly we may refer to $\theta$ as either a 'quantile level' or a '(cumulative) probability'. Additionally, we will use $f(x)$ to denote a probability mass function (PMF) for a prediction of a discrete variable or a discretization (such as binned values) of a continuous variable.

The quantile average combines a set of quantile functions, $\mathcal{Q} = \{F_i^{-1}(\theta)| i \in 1,...,N \}$, with a given set of weights, $\pmb{w}$, as $$
F^{-1}_Q(\theta) = C_Q(\mathcal{Q}, \pmb{w}) = \sum_{i = 1}^Nw_iF^{-1}_i(\theta).
$$This computes the average value of predictions across different models for each fixed quantile level $\theta$. It is also possible to use other combination functions, such as a weighted median, to combine quantile predictions.

The probability average or linear pool is calculated by averaging probabilities across predictions for a fixed value of the target variable, $x$. In other words, for a set of CDFs, $\mathcal{F} = \{F_i(x)| i \in 1,...,N \}$ and weights, $\pmb{w}$, the linear pool is calculated as

$$
F_{LOP}(x) = C_{LOP}(\mathcal{F}, \pmb{w}) = \sum_{i = 1}^Nw_iF_i(x). 
$$ 

For a set of PMFs, $\{f_i|i \in 1, ..., N\}$, the linear pool can be equivalently calculated: $f_{LOP}(x) = \sum_{i = 1}^N w_i f_i(x)$.

The different averaging methods for probabilistic predictions yield different properties of the resulting ensemble distribution. For example, the variance of the linear pool is $\sigma^2_{LOP} = \sum_{i=1}^Nw_i\sigma_i^2 + \sum_{i=1}^Nw_i(\mu_i-\mu_{LOP})^2$, where $\mu_i$ is the mean and $\sigma^2_i$ is the variance of individual prediction $i$, and although there is no closed-form variance for the quantile average, the variance of the quantile average will always be less than or equal to that of the linear pool [@lichtendahl2013]. Both methods generate distributions with the same mean, $\mu_Q = \mu_{LOP} = \sum_{i=1}^Nw_i\mu_i$, which is the mean of individual model means [@lichtendahl2013]. The linear pool method preserves variation between individual models, whereas the quantile average cancels away this variation under the assumption it constitutes sampling error [@howerton2023].

## Model implementation details {#sec-implementation}

To understand how these methods are implemented in [hubEnsembles]{.pkg}, we first must define the conventions employed by the hubverse and its packages for representing and working with model predictions. We begin with a short overview of concepts and conventions needed to utilize the [hubEnsembles]{.pkg} package, then explain the implementation of the two ensembling functions provided by the package, `simple_ensemble` and `linear_pool`.

### Hubverse terminology and conventions

A central concept in the hubverse effort is "model output". Model output is a specially formatted tabular representation of predictions. Each row represents a single, unique prediction with each column providing information about what is being predicted, its scope, and its value. Per hubverse convention, each column serves one of three purposes: denote which model has produced the prediction (called the "model ID"), provide details about what is being predicted (called the "task IDs"), or specify how the prediction is represented (called the "model output representation") [@hubverse_docs]. 

Predictions are assumed to be generated by distinct models, typically developed and run by a modeling team of one or more individuals. Each model should have a unique identifier that is stored in the `model_id` column. Then, the details of the outcome being predicted can be stored in a series of task ID columns. These task ID columns may also include additional information, such as any conditions or assumptions that were used to generate the predictions [@hubverse_docs]. For example, short-term forecasts of incident influenza hospitalizations in the US at different locations and amounts of time in the future might represent this information using a `target` column with the value "wk ahead inc flu hosp", a `location` column identifying the location being predicted, a `reference_date` column with the "starting point" of the forecasts, and a `horizon` column with the number of steps ahead that the forecast is predicting relative to the `reference_date` (@tbl-flu-forecasts). All these variables make up the task ID columns.

```{r flu-forecasts, echo=FALSE}
#| label: tbl-flu-forecasts
#| tbl-cap: "Example of forecasts for weekly incident flu hospitalizations,
#| formatted according to hubverse standards. Quantile forecasts for the median
#| and 50%, 80%, and 98% prediction intervals are shown from a single model.
#| The `location` and `reference_date` columns have been omitted for brevity;
#| all forecasts in this example were made on 2023-05-15 for the US."

readr::read_rds(
  here::here("analysis/data/raw_data/flu_forecasts_raw.rds")
) |>
  tidyr::separate(target,
    sep = " ", convert = TRUE,
    into = c("horizon", "target"), extra = "merge"
  ) |>
  as_model_out_tbl(
    model_id_col = "model",
    output_type_col = "class",
    output_type_id_col = "quantile",
    value_col = "value",
    sep = "-",
    trim_to_task_ids = FALSE,
    hub_con = NULL,
    task_id_cols = c(
      "timezero", "location", "horizon",
      "target"
    ),
    remove_empty = TRUE
  ) |>
  dplyr::filter(
    unit == "US",
    model_id == "UMass-trends_ensemble",
    output_type_id %in% c(0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 0.99)
  ) |>
  dplyr::rename(reference_date = timezero, location = unit) |>
  dplyr::select(-season) |>
  dplyr::select(
    model_id, target, horizon,
    output_type, output_type_id, value
  ) |>
  head(n = 7L) |>
  knitr::kable()
```

Alternatively, longer-term scenario projections may require different task ID columns. For example, projections of cumulative COVID-19 deaths in the US at different locations, amounts of time in the future, and under different assumed conditions may use a `target` column of "cum death", a `location` column specifying the location being predicted, an `origin_date` column specifying the date on which the projections were made, a `horizon` column describing the number of steps ahead that the projection is predicting relative to the `origin_date`, and a `scenario_id` column denoting the future conditions that were modeled and are projected to result in the specified number of cumulative deaths (@tbl-example-scenarios). Different modeling efforts may use different sets of task ID columns and values to specify their prediction goals. Additional examples of task ID variables are available on the hubverse documentation website [@hubverse_docs].

```{r example-scenarios, echo=FALSE}
#| label: tbl-example-scenarios
#| tbl-cap: "Example of scenario projections for cumulative COVID-19 deaths,
#| formatted according to hubverse standards. Quantile predictions for the
#| median and 50% prediction intervals from a single model are shown for four
#| distinct scenarios. The `location` and `origin_date` columns have been
#| omitted for brevity; all forecasts in this example were made on 2021-03-07
#| for the US. This example is a subset of the `example-complex-scenario-hub`
#| data provided by the hubverse[@hubverse_docs]."
readr::read_rds(
  here::here("analysis/data/example_data/example_scenario_output.rds")
) |>
  dplyr::select(
    model_id, target, horizon, scenario_id,
    output_type, output_type_id, value
  ) |>
  knitr::kable()
```

The third purpose of a model output column is to specify the models prediction and details about how it is represented. This "model output representation" includes the predicted values along with metadata that specifies how the predictions are conveyed and always consists of the same three columns: (1) `output_type`, (2) `output_type_id`, and (3) `value`. The `output_type` column defines how the prediction is represented and may be one of `"mean"` or `"median"` (point prediction), `"quantile"`, `"cdf"`, `"pmf"` (probabilistic prediction), or `"sample"` (although this output type is not yet supported by the [hubEnsembles]{.pkg} package). The `output_type_id` provides additional identifying information for a prediction and is specific to the particular `output_type` (see @tbl-flu-forecasts). For quantile predictions, the `output_type_id` is a numeric value between 0 and 1 specifying the cumulative probability associated with the quantile prediction. In the notation we defined above, the `output_type_id` corresponds to $\theta$ and the `value` is the quantile prediction $F^{-1}(\theta)$. For CDF or PMF predictions, the `output_type_id` is the target variable value $x$ at which the cumulative distribution function or probability mass function for the predictive distribution should be evaluated, and the `value` column contains the predicted $F(x)$ or $f(x)$, respectively. Requirements for the values of the `output_type_id` and `value` columns associated with each valid output type are summarized in @tbl-model-output-rep. 

This representation of predictive model output is codified by the `model_out_tbl` S3 class in the [hubUtils]{.pkg} package, one of the foundational hubverse packages. Although this S3 class is required for all [hubEnsembles]{.pkg} functions, model predictions in other formats can easily be transformed using the `as_model_out_tbl()` function from `hubUtils`. An example of this transformation is provided in @sec-flu.

| `output_type` | `output_type_id`                                                                            | `value`                                                                                                                                                                              |
|:--------------|:-----------------------|:--------------------------------|
| `mean`        | NA (not used for mean predictions)                                                          | Numeric: The mean of the predictive distribution                                                                                                                                     |
| `median`      | NA (not used for median predictions)                                                        | Numeric: The median of the predictive distribution                                                                                                                                   |
| `quantile`    | Numeric between 0.0 and 1.0: A quantile level                                            | Numeric: The quantile of the predictive distribution at the quantile level specified by the `output_type_id`                                                                      |
| `cdf`         | Numeric within the support of the outcome variable: a possible value of the target variable | Numeric between 0.0 and 1.0: The cumulative probability of the predictive distribution at the value of the outcome variable specified by the `output_type_id` |
| `pmf`         | String naming a possible category of a discrete outcome variable                            | Numeric between 0.0 and 1.0: The probability mass of the predictive distribution when evaluated at a specified level of a categorical outcome variable         |
| `sample`      | Positive integer sample index                                                               | Numeric: A sample from the predictive distribution                                                                                                                                   |

: A table summarizing how the model output representation columns are used for predictions of different output types. Adapted from [@hubverse_docs] {#tbl-model-output-rep}

## Ensemble functions in hubEnsembles {#sec-ens-fns}

The [hubEnsembles]{.pkg} package includes two functions that perform ensemble calculations: `simple_ensemble()`, which applies some function to each model prediction, and `linear_pool()`, which computes an ensemble using the linear opinion pool method. In the following sections, we outline the implementation details for each function and how these implementations correspond to the statistical ensembling methods described in @sec-defs. A short description of the calculation performed by each function is summarized by output type in @tbl-fns-by-output-type.

#### Simple ensemble

The `simple_ensemble` function directly computes an ensemble from component model outputs by combining them via some function ($C$) within each unique combination of task ID variables, output types, and output type IDs. This function can be used to summarize predictions of output types mean, median, quantile, CDF, and PMF. The mechanics of the ensemble calculations are the same for each of the output types, though the resulting statistical ensembling method differs for different output types (@tbl-fns-by-output-type). 

By default, `simple_ensemble` uses the mean for the aggregation function $C$ and equal weights for all models. For point predictions with a mean or median output type, the resulting ensemble prediction is an equally weighted average of the individual models' predictions. For probabilistic predictions in a quantile format, by default `simple_ensemble` calculates an equally weighted average of individual model target variable values at each quantile level, which is equivalent to a quantile average. For model outputs in a CDF or PMF format, by default `simple_ensemble` computes an equally weighted average of individual model (cumulative or bin) probabilities at each target variable value, which is equivalent to the linear pool method.

Any aggregation function $C$ may be specified by the user. For example, a median ensemble may also be created by specifying "median" as the aggregation function, or a custom function may be passed to the `agg_fun` argument to create other ensemble types. Similarly, model weights can be specified to create a weighted ensemble.

| `output_type` | `simple_ensemble(..., agg_fun = "mean")` | `linear_pool()`|
|-------------|------------------------|----------------------------------|
| `mean` | mean of individual model means | mean of individual model means |
| `median` | mean of individual model medians | NA |
| `quantile` | mean of individual model target variable values at each quantile level, $F^{-1}_Q(\theta)$ | quantile of the distribution obtained by computing the mean of estimated individual model cumulative probabilities at each target variable value, $F^{-1}_{LOP}(x)$ |
| `cdf` | mean of individual model cumulative probabilities at each target variable value, $F_{LOP}(x)$ | mean of individual model cumulative probabilities at each target variable value, $F_{LOP}(x)$ |
| `pmf` | mean of individual model bin probabilities at each target variable value, $f_{LOP}(x)$| mean of individual model bin probabilities at each target variable value, $f_{LOP}(x)$|

: Summary of ensemble function calculations for each output type. The ensemble function (columns) determines the operation that is performed, and in the case of probabilistic output types (`quantile`, `cdf`, `pmf`), this also determines what ensemble distribution is generated (quantile average, $F_{Q}^{-1}(\theta)$, or linear pool, $F_{LOP}(x)$). The resulting ensemble will be returned in the same output type as the inputs. Thus, the output type (rows) determines how the resulting ensemble distribution is summarized (as a quantile function, $F^{-1}(\theta)$, cumulative distribution function, $F(x)$, or probability mass function $f(x)$). Estimating individual model cumulative probabilities is required to compute a `linear_pool()` for predictions of `quantile` output type; see @sec-linear-pool for details. In the case of `simple_ensemble()`, we report the calculations for the default case where `agg_fun = "mean"`; however, if another aggregation function is chosen (e.g., `agg_fun = "median"`), that calculation would be performed instead. For example, `simple_ensemble(..., agg_fun = "median")` applied to predictions of `mean` output type would return the median of individual model means. {#tbl-fns-by-output-type}

#### Linear pool {#sec-linear-pool}

The `linear_pool` function implements the linear opinion pool method for ensembling predictions. This function can be used to combine predictions with output types mean, quantile, CDF, and PMF. Unlike `simple_ensemble`, this function handles its computation differently based on the output type. For the CDF, PMF, and mean output types, the linear pool method is equivalent to calling `simple_ensemble` with a mean aggregation function (see @tbl-fns-by-output-type), since `simple_ensemble` produces a linear pool prediction (an average of individual model cumulative or bin probabilities).

However, implementation of LOP is less straightforward for the quantile output type. This is because LOP averages cumulative probabilities at each value of the target variable, but the predictions are quantiles (on the scale of the target variable) for fixed quantile levels. The value for these quantile predictions will generally differ between models, and as a result we are typically not provided cumulative probabilities at the same values of the target variable for all component predictions. This lack of alignment between cumulative probabilities for the same target variable values impedes computation of LOP from quantile predictions and is illustrated in panel A of @fig-example-quantile-average-and-linear-pool.

```{r example-quantile-average-and-linear-pool, echo=FALSE}
#| label: fig-example-quantile-average-and-linear-pool
#| fig-cap: "(Panel A) Example of quantile output type predictions. In this
#| example, points show model output collected for seven fixed quantile levels
#| ($\\theta$ = 0.01, 0.1, 0.3, 0.5, 0.7, 0.9, and 0.99) from two distributions
#| ($N(100, 10)$ in purple and $N(120, 5)$ in green), with the underlying
#| cumulative distribution functions (CDFs) shown with curves. The associated
#| values for each fixed quantile level do not align across distributions
#| (vertical lines). (Panel B) Quantile average ensemble, which is calculated by
#| averaging values for each fixed quantile level (represented by horizontal
#| dashed gray lines). The distributions and corresponding model outputs from
#| panel A are re-plotted and the black line shows the resulting quantile
#| average ensemble. Inset shows corresponding probability density functions
#| (PDFs). (Panel C) Linear pool ensemble, which is calculated by averaging
#| cumulative probabilities for each fixed value (represented by vertical
#| dashed gray lines). The distributions and corresponding model outputs from
#| panel A are re-plotted. To calculate the linear pool in this case, where
#| model outputs are not defined for the same values, the model outputs are used
#| to interpolate the full CDF for each distribution from which quantiles can be
#| extracted for fixed values (shown with open circles). The black line shows
#| theresulting linear pool average ensemble. Inset shows corresponding PDFs."
#| fig-width: 10
#| fig-height: 4

# set quantiles and values over which to define distributions
quantile_probs <- c(0.01, seq(from = 0.1, to = 0.9, by = 0.2), 0.99)
lop_example_x <- seq(85, 135, 12)
x <- unique(c(seq(from = 0.0, to = 400.0, length = 1001), lop_example_x))

# distribution 1 ~ N(100, 10)
mean1 <- 100
sd1 <- 10
# distribution 2 ~ N(120, 5)
mean2 <- 120
sd2 <- 5
# distribution colors
dist_colors <- viridisLite::viridis(2, end = 0.9)

# cdfs defined for fixed quantiles
cdf_defined_on_quantiles <-
  data.frame(
    quantile = rep(quantile_probs, 3),
    value = c(
      qnorm(quantile_probs, mean1, sd1),
      qnorm(quantile_probs, mean2, sd2),
      qnorm(
        quantile_probs, mean(c(mean1, mean2)),
        mean(c(sd1, sd2))
      )
    ),
    distribution = c(
      rep("A", length(quantile_probs)),
      rep("B", length(quantile_probs)),
      rep("Q", length(quantile_probs))
    )
  )

# cdfs defined for fixed values
cdf_defined_on_values <-
  data.frame(
    value = rep(x, 4),
    quantile = c(
      pnorm(x, mean1, sd1), pnorm(x, mean2, sd2),
      rowMeans(cbind(
        pnorm(x, mean1, sd1),
        pnorm(x, mean2, sd2)
      )),
      pnorm(x, mean(c(mean1, mean2)), mean(c(sd1, sd2)))
    ),
    distribution = c(
      rep("A", length(x)), rep("B", length(x)),
      rep("LOP", length(x)), rep("Q", length(x))
    )
  )

# pdfs for all distributions
pdfs_defined_on_values <-
  data.frame(
    value = rep(x, 4),
    prob = c(
      dnorm(x, mean = mean1, sd = sd1),
      dnorm(x, mean = mean2, sd = sd2),
      rowMeans(cbind(
        dnorm(x, mean = mean1, sd = sd1),
        dnorm(x, mean = mean2, sd = sd2)
      )),
      dnorm(x,
        mean = mean(c(mean1, mean2)),
        sd = mean(c(sd1, sd2))
      )
    ),
    distribution = c(
      rep("A", length(x)), rep("B", length(x)),
      rep("LOP", length(x)), rep("Q", length(x))
    )
  )

### panel A
pa <- ggplot(
  data = dplyr::filter(
    cdf_defined_on_values,
    distribution %in% c("A", "B")
  ),
  aes(x = value, y = quantile, color = distribution)
) +
  geom_line(linewidth = 0.8, alpha = 0.3) +
  geom_segment(
    data = dplyr::filter(
      cdf_defined_on_quantiles,
      distribution %in% c("A", "B")
    ),
    aes(x = value, xend = value, y = 0, yend = quantile),
    alpha = 0.5
  ) +
  geom_point(data = dplyr::filter(
    cdf_defined_on_quantiles,
    distribution %in% c("A", "B")
  )) +
  coord_cartesian(xlim = c(75, 140)) +
  labs(
    x = "value", y = "cumulative probability (quantile)",
    subtitle = "quantile predictions from two distributions"
  ) +
  scale_color_manual(
    name = "distributions",
    labels = c("N(100, 10)", "N(120, 5)"),
    values = dist_colors
  ) +
  scale_y_continuous(
    breaks = quantile_probs, limits = c(0, 1),
    expand = c(0, 0)
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank()
  )

### panel B
pb <- ggplot(
  data = dplyr::filter(cdf_defined_on_values, distribution != "LOP"),
  mapping = aes(x = value, y = quantile)
) +
  geom_segment(
    data = dplyr::filter(
      cdf_defined_on_quantiles,
      distribution != "LOP"
    ) |>
      reshape2::dcast(quantile ~ distribution, value.var = "value"),
    mapping = aes(x = A, xend = B, y = quantile, yend = quantile),
    color = "darkgray", linetype = "dashed"
  ) +
  geom_line(mapping = aes(
    alpha = distribution, color = distribution,
    linewidth = distribution
  )) +
  geom_point(
    data = dplyr::filter(
      cdf_defined_on_quantiles,
      distribution %in% c("A", "B")
    ),
    mapping = aes(color = distribution, size = distribution)
  ) +
  coord_cartesian(xlim = c(75, 135)) +
  labs(x = "value", y = "", subtitle = "quantile average ensemble") +
  scale_alpha_manual(values = c(0.3, 0.3, 1)) +
  scale_color_manual(values = c(dist_colors, "black")) +
  scale_linewidth_manual(values = c(0.8, 0.8, 1.2)) +
  scale_size_manual(values = c(1.2, 1.2, 1.7)) +
  scale_y_continuous(
    breaks = quantile_probs, limits = c(0, 1),
    expand = c(0, 0)
  ) +
  theme_bw() +
  theme(
    legend.position = "none", panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank()
  )

pb_inset <- ggplot(
  data = dplyr::filter(
    pdfs_defined_on_values,
    distribution != "LOP"
  ),
  mapping = aes(x = value, y = prob, color = distribution)
) +
  geom_line(aes(alpha = distribution, linewidth = distribution)) +
  coord_cartesian(xlim = c(75, 135)) +
  scale_alpha_manual(values = c(0.8, 0.8, 1)) +
  scale_color_manual(values = c(dist_colors, "black")) +
  scale_linewidth_manual(values = c(0.8, 0.8, 1.2)) +
  theme_bw() +
  theme(
    axis.text.y = element_blank(), axis.ticks.y = element_blank(),
    axis.title = element_blank(), legend.position = "none",
    panel.grid = element_blank()
  )

pb_fin <- cowplot::ggdraw(pb) +
  cowplot::draw_plot(
    pb_inset + theme(plot.margin = unit(
      0.01 * c(1, 1, 1, 1),
      "cm"
    )),
    .19, .6, .3, .3,
    hjust = 0, vjust = 0
  )


### panel C
pc <- ggplot(
  data = cdf_defined_on_values |> dplyr::filter(distribution != "Q"),
  mapping = aes(x = value, y = quantile)
) +
  geom_segment(
    data = cdf_defined_on_values |>
      dplyr::filter(distribution != "Q", value %in% lop_example_x) |>
      reshape2::dcast(value ~ distribution, value.var = "quantile"),
    mapping = aes(x = value, xend = value, y = A, yend = B),
    color = "darkgray", linetype = "dashed"
  ) +
  geom_line(mapping = aes(
    alpha = distribution, color = distribution,
    linewidth = distribution
  )) +
  geom_point(
    data = dplyr::bind_rows(
      cdf_defined_on_values |>
        dplyr::filter(
          distribution %in% c("A", "B"),
          value %in% lop_example_x
        ) |>
        dplyr::mutate(point_type = "interpolation"),
      dplyr::filter(
        cdf_defined_on_quantiles,
        distribution %in% c("A", "B")
      ) |>
        dplyr::mutate(point_type = "model output")
    ),
    mapping = aes(
      color = distribution, size = distribution,
      shape = point_type
    ),
  ) +
  coord_cartesian(xlim = c(75, 135)) +
  guides(alpha = "none", color = "none", linewidth = "none", size = "none") +
  labs(x = "value", y = "", subtitle = "linear pool ensemble") +
  scale_alpha_manual(values = c(0.3, 0.3, 1)) +
  scale_color_manual(values = c(dist_colors, "black")) +
  scale_linewidth_manual(values = c(0.8, 0.8, 1.2)) +
  scale_shape_manual(values = c(1, 19), name = "") +
  scale_size_manual(values = c(1.2, 1.2, 1.9)) +
  scale_y_continuous(
    breaks = quantile_probs, limits = c(0, 1),
    expand = c(0, 0)
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom", panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank()
  )

pc_inset <- ggplot(
  data = pdfs_defined_on_values |>
    dplyr::filter(distribution != "Q"),
  mapping = aes(x = value, y = prob, color = distribution)
) +
  geom_line(aes(alpha = distribution, linewidth = distribution)) +
  coord_cartesian(xlim = c(75, 135)) +
  scale_alpha_manual(values = c(0.8, 0.8, 1)) +
  scale_color_manual(values = c(dist_colors, "black")) +
  scale_linewidth_manual(values = c(0.8, 0.8, 1.2)) +
  theme_bw() +
  theme(
    axis.text.y = element_blank(), axis.ticks.y = element_blank(),
    axis.title = element_blank(), legend.position = "none",
    panel.grid = element_blank()
  )

pc_fin <- cowplot::ggdraw(pc + theme(legend.position = "none")) +
  cowplot::draw_plot(
    pc_inset + theme(plot.margin = unit(
      0.01 * c(1, 1, 1, 1),
      "cm"
    )),
    .19, .6, .3, .3,
    hjust = 0, vjust = 0
  )

# combine into final figure
l <- cowplot::get_legend(pa)
l2 <- cowplot::get_legend(pc)

cowplot::plot_grid(
  cowplot::plot_grid(
    pa + theme(legend.position = "none"), pb_fin, pc_fin,
    labels = LETTERS[1:3], nrow = 1, rel_widths = c(0.34, 0.33, 0.33)
  ),
  cowplot::plot_grid(
    "", l, l2, "",
    nrow = 1, rel_widths = c(0.175, 0.325, 0.325, 0.175)
  ),
  ncol = 1, rel_heights = c(0.95, 0.05)
)
```

Given that LOP cannot be directly calculated from quantile predictions, we must first obtain an estimate of the CDF for each component distribution using the provided quantiles, combine the CDFs, then calculate the quantiles from the ensemble's CDF. We perform this calculation in three main steps, assisted by the [distfromq]{.pkg} package [@distfromq] for the first two:

1.  Interpolate and extrapolate from the provided quantiles for each component model to obtain an estimate of the CDF of that particular distribution.
2.  Draw samples from each component model distribution. To reduce Monte Carlo variability, we use quasi-random samples corresponding to quantiles of the estimated distribution [@niederreiter1992quasirandom].
3.  Pool the samples from all component models and extract the desired quantiles.

For step 1, functionality in the [distfromq]{.pkg} package uses a monotonic cubic spline for interpolation on the interior of the provided quantiles. The user may choose one of several distributions to perform extrapolation of the CDF tails. These include normal, lognormal, and cauchy distributions, with "normal" set as the default. A location-scale parameterization is used, with separate location and scale parameters chosen in the lower and upper tails so as to match the two most extreme quantiles. The sampling process described in steps 2 and 3 approximates the linear pool calculation described in @sec-defs.

## Demonstration of functionality {#sec-simple-ex}

In this section, we provide a simple example to illustrate the two main functions in [hubEnsembles]{.pkg}, `simple_ensemble()` and `linear_pool()`. 

### Example data: a forecast hub

We will use an example hub provided by the hubverse to demonstrate the functionality of the [hubEnsembles]{.pkg} package [@hubverse_docs]. The example hub includes both example model output data and target data (sometimes known as "truth" data), which are included in the [hubEnsembles]{.pkg} package as data objects named `example_model_output` and `example_target_data`. 

The model output data includes `quantile`, `mean` and `median` forecasts of future incident influenza hospitalizations and `pmf` forecasts of hospitalization intensity. Each forecast is made for five task ID variables, including the location for which the forecast was made (`location`), the date on which the forecast was made (`reference_date`), the number of steps ahead (`horizon`), the date of the forecast prediction (a combination of the date the forecast was made and the forecast horizon, `target_end_date`), and the forecast target (`target`). @tbl-example-forecasts provides an example set of quantile forecasts included in this example model output. In @tbl-example-forecasts, we show only the median, the 50%, and 95% prediction intervals, although other intervals and `mean` forecasts are included in the example model output data.

```{r, cols.print=12}
#| label: tbl-example-forecasts
#| prompt: true
#| tbl-cap: "Example model output for forecasts of incident influenza
#| hospitalizations. A subset of example model output is shown: 1-week ahead
#| quantile forecasts made on 2022-12-17 for the US from three distinct models;
#| only the median, 50% prediction intervals, and 95% prediction intervals are
#| displayed. The `location`, `reference_date` and `target_end_date` columns
#| have been omitted for brevity. This example data is provided in the
#| [hubEnsembles]{.pkg} package and is a subset of the
#| `example-complex-forecast-hub` data provided by the 
#| hubverse [@hubverse_docs]."
hubEnsembles::example_model_output |>
  dplyr::filter(
    output_type %in% c("quantile", "median"),
    output_type_id %in% c(0.025, 0.25, 0.75, 0.75, 0.975, NA),
    reference_date == "2022-12-17",
    location == "US",
    horizon == 1
  ) |>
  dplyr::select(-location, -target_end_date, -reference_date) |>
  knitr::kable()
```

We also have corresponding target data included in the [hubEnsembles]{.pkg} package (@tbl-example-target-data). The example target data provide observed incident influenza hospitalizations (`value`) in a given week (`time_idx`) and for a given location (`location`). This target data could be used as calibration data for generating forecasts or for evaluating these forecasts post hoc. The forecast-specific task ID variables `reference_date` and `horizon` are not relevant for the target data.

```{r, cols.print=13}
#| label: tbl-example-target-data
#| prompt: true
#| tbl-cap: "Example target data for incident influenza hospitalizations. This
#| table includes target data from 2022-11-01 and 2023-02-01. This target data
#| is provided in the [hubEnsembles]{.pkg} package and is a subset of the
#| `example-complex-forecast-hub` target data provided by the
#| hubverse [@hubverse_docs]."
hubEnsembles::example_target_data |>
  dplyr::filter(
    location == "US",
    time_idx >= "2022-11-01",
    time_idx <= "2023-02-01"
  ) |>
  knitr::kable()
```

We can plot these forecasts and the target data using the `plot_step_ahead_model_output()` function from [hubVis]{.pkg}, another package for visualizing model outputs from the hubverse suite (@fig-plot-ex-mods). We subset the model output data and the target data to the location and time horizons we are interested in. 

```{r plot-ex-mods}
#| label: fig-plot-ex-mods
#| prompt: true
#| fig-cap: "One example quantile forecast of weekly US incident influenza
#| hospitalizations is shown for each of three models (panels). Forecasts are
#| represented by a median (line), 50% prediction interval (Q25-Q75), and 95%
#| prediction interval (Q2.5-Q97.5). Gray points represent observed incident
#| hospitalizations."
#| fig-width: 8
#| fig-height: 4

model_outputs_plot <- hubEnsembles::example_model_output |>
  hubUtils::as_model_out_tbl() |>
  dplyr::filter(
    location == "US",
    output_type %in% c("median", "mean", "quantile"),
    reference_date == "2022-12-17"
  )
target_data_plot <- hubEnsembles::example_target_data |>
  dplyr::filter(
    location == "US",
    time_idx >= "2022-11-01",
    time_idx <= "2023-02-01"
  )
hubVis::plot_step_ahead_model_output(
  model_output_data = model_outputs_plot,
  truth_data = target_data_plot,
  facet = "model_id",
  facet_nrow = 1,
  interactive = FALSE,
  intervals = c(0.5, 0.95),
  show_legend = FALSE,
  use_median_as_point = TRUE,
  x_col_name = "target_end_date"
) +
  theme_bw() +
  labs(y = "US incident hospitalizations")
```

Next, we examine the `pmf` target in the example model output data. For this target, teams forecasted the probability that hospitalization intensity will be "low", "moderate", "high", or "very high". These hospitalization intensity categories are determined by thresholds for weekly hospital admissions per 100,000 population. In other words, "low" hospitalization intensity in a given week means few incident influenza hospitalizations per 100,000 population are predicted, whereas "very high" hospitalization intensity means many hospitalizations per 100,000 population are predicted. These forecasts are made for the same task ID variables as the `quantile` forecasts of incident hospitalizations. 

We show a representative example of the hospitalization intensity category forecasts in @tbl-example-forecasts-pmf. Because these forecasts are `pmf` output type, the `output_type_id` column specifies the bin of hospitalization intensity and the `value` column provides the forecasted probability of hospitalization incidence being in that category. Values sum to 1 across bins. For the MOBS-GLEAM_FLUH and PSI-DICE models, incidence is forecasted to decrease over the horizon (@fig-plot-ex-mods), and correspondingly, there is lower probability of "high" and "very high" hospitalization intensity for later horizons (@fig-plot-ex-mods-pmf).

```{r, cols.print=9}
#| label: tbl-example-forecasts-pmf
#| prompt: true
#| tbl-cap: "Example `pmf` model output for forecasts of incident influenza
#| hospitalization intensity. A subset of example model output is shown: 1-week
#| ahead pmf forecasts made on 2022-12-17 for the US from three distinct models.
#| We round the forecasted probability (in the `value` column) to two digits.
#| The `location`, `reference_date` and `target_end_date` columns have been
#| omitted for brevity. This example data is provided in the
#| [hubEnsembles]{.pkg} package and is a subset of the
#| `example-complex-forecast-hub` data provided by the
#| hubverse [@hubverse_docs]."
hubEnsembles::example_model_output |>
  dplyr::filter(
    output_type %in% c("pmf"),
    reference_date == "2022-12-17",
    location == "US",
    horizon == 1
  ) |>
  dplyr::mutate(value = round(value, 2)) |>
  dplyr::select(-location, -reference_date, -target_end_date) |>
  knitr::kable()
```

```{r}
#| echo: FALSE
#| label: fig-plot-ex-mods-pmf
#| fig-cap: "One example `pmf` forecast of incident influenza hospitalization
#| intensity is shown for each of three models (panels). Each bar shows
#| forecasts of horizon 0-3 weeks, and the darkness of the bar shows the
#| hospitalization intensity bins (low, moderate, high, and very high)."
#| fig-width: 8
#| fig-height: 4

model_outputs_plot_pmf <- hubEnsembles::example_model_output |>
  hubUtils::as_model_out_tbl() |>
  dplyr::filter(
    location == "US",
    output_type %in% c("pmf"),
    reference_date == "2022-12-17"
  ) |>
  dplyr::mutate(
    output_type_id = factor(output_type_id,
      levels = c("low", "moderate", "high", "very high")
    )
  )

ggplot(
  data = model_outputs_plot_pmf,
  aes(x = horizon, y = value, fill = model_id, alpha = output_type_id)
) +
  geom_bar(stat = "identity", position = "stack") +
  facet_grid(cols = vars(model_id)) +
  guides(fill = "none") +
  scale_alpha_discrete(name = "hospitalization intensity") +
  scale_fill_brewer(palette = "Set2") +
  scale_x_continuous(expand = c(0, 0), name = "horizon (weeks)") +
  scale_y_continuous(expand = c(0, 0), name = "bin probability") +
  theme_bw() +
  theme(
    legend.position = "bottom",
    panel.grid = element_blank()
  )
```


### Creating ensembles with simple_ensemble

Using the default options for `simple_ensemble()`, we can generate an equally weighted mean ensemble for each unique combination of values for the task ID variables, the `output_type` and the `output_type_id`. Recall that this means different ensemble methods will be used for different output types: for the `quantile` output type in our example data, the resulting ensemble is a quantile average, while for the `pmf` output type, the ensemble is a linear pool.

```{r}
#| prompt: true
mean_ens <- hubEnsembles::simple_ensemble(
  hubEnsembles::example_model_output,
  model_id = "simple-ensemble-mean"
)
```

The resulting model output has the same structure as the original model output data (@tbl-mean-ensemble), with columns for model ID, task ID variables, output type, output type ID, and value. We also use `model_id = "simple-ensemble-mean` to change the name of this ensemble in the resulting model output; if not specified, the default will be "hub-ensemble".

```{r, rows.print = 9}
#| label: tbl-mean-ensemble
#| prompt: true
#| tbl-cap: "Mean ensemble model output. The values in the model_id column are
#| determined by `simple_ensemble(..., model_id = )` argument). A subset of
#| ensemble model output is shown: 1-week ahead pmf forecasts made on 2022-12-17
#| for the US. Results are generated for all output types. Here, we show only
#| the median, 50% prediction intervals, and 95% prediction intervals for the
#| quantile output type and all bins for the pmf output type. The `location`,
#| `reference_date` and `target_end_date` columns have been omitted for brevity,
#| and the `value` column is rounded to two digits."
mean_ens |>
  dplyr::filter(
    output_type %in% c("quantile", "median", "pmf"),
    output_type_id %in% c(
      0.025, 0.25, 0.75, 0.975, NA,
      "low", "moderate", "high", "very high"
    ),
    reference_date == "2022-12-17",
    location == "US",
    horizon == 1
  ) |>
  dplyr::mutate(value = round(value, 2)) |>
  dplyr::select(-location, -reference_date, -target_end_date) |>
  knitr::kable()
```

#### Changing the aggregation function

We can change the function that is used to aggregate model outputs. For example, we may want to calculate a median of the component models' submitted values for each quantile. We do so by specifying `agg_fun = median`.

```{r}
#| prompt: true
median_ens <- hubEnsembles::simple_ensemble(hubEnsembles::example_model_output,
  agg_fun = median,
  model_id = "simple-ensemble-median"
)
```

Custom functions can also be passed into the `agg_fun` argument. We illustrate this by defining a custom function to compute the ensemble prediction as a geometric mean of the component model predictions. Any custom function to be used must have an argument `x` for the vector of numeric values to summarize, and if relevant, an argument `w` of numeric weights.

```{r}
#| prompt: true
geometric_mean <- function(x) {
  n <- length(x)
  return(prod(x)^(1 / n))
}
geometric_mean_ens <-
  hubEnsembles::simple_ensemble(hubEnsembles::example_model_output,
    agg_fun = geometric_mean,
    model_id = "simple-ensemble-geometric"
  )
```

As expected, the mean, median, and geometric mean each give us slightly different resulting ensembles. The median point estimates, 50% prediction intervals, and 95% prediction intervals in @fig-plot-ensembles demonstrate this.

```{r plot-ensembles}
#| label: fig-plot-ensembles
#| prompt: true
#| fig-cap: "Three different ensembles for weekly US incident influenza
#| hospitalizations. Each ensemble combines individual predictions from the
#| example hub (@fig-plot-ex-mods) using a different method: arithmetic mean,
#| geometric mean, or median. All methods correspond to variations of the
#| quantile average approach."
#| fig-height: 4
#| fig-width: 8

model_output_plot <- dplyr::bind_rows(
  mean_ens, median_ens,
  geometric_mean_ens
) |>
  dplyr::filter(
    location == "US",
    output_type %in% c("median", "mean", "quantile"),
    reference_date == "2022-12-17"
  ) |>
  dplyr::mutate(target_date = reference_date + horizon)
target_data_plot <- hubEnsembles::example_target_data |>
  dplyr::filter(
    location == "US", time_idx >= "2022-11-01",
    time_idx <= "2023-03-01"
  )
hubVis::plot_step_ahead_model_output(
  model_output_data = model_output_plot,
  truth_data = target_data_plot,
  use_median_as_point = TRUE,
  interactive = FALSE,
  intervals = c(0.5, 0.95),
  show_legend = TRUE,
  x_col_name = "target_end_date"
) +
  theme_bw() +
  labs(y = "US incident hospitalizations")
```

#### Weighting model contributions

We can weight the contributions of each model in the ensemble using the `weights` argument of `simple_ensemble`. This arguement takes a `data.frame` that should include a `model_id` column containing each unique model ID and a `weight` column. In the following example, we include the baseline model in the ensemble, but give it less weight than the other forecasts.

```{r}
#| prompt: true
model_weights <- data.frame(
  model_id = c(
    "MOBS-GLEAM_FLUH", "PSI-DICE",
    "simple_hub-baseline"
  ),
  weight = c(0.4, 0.4, 0.2)
)
weighted_mean_ens <-
  hubEnsembles::simple_ensemble(
    hubEnsembles::example_model_output,
    weights = model_weights,
    model_id = "simple-ensemble-weighted-mean"
  )
```

### Creating ensembles with linear_pool

We can also generate a linear pool ensemble, or distributional mixture, using the `linear_pool()` function; this function can be applied to predictions with an `output_type` of `mean`, `quantile`, `cdf`, or `pmf`. Our example hub includes `median` output type, so we exclude it from the calculation.

```{r}
#| prompt: true
linear_pool_ens <-
  hubEnsembles::linear_pool(
    dplyr::filter(
      hubEnsembles::example_model_output,
      output_type != "median"
    ),
    model_id = "linear-pool"
  )
```

As described above, for `quantile` model outputs, the `linear_pool` function approximates the full probability distribution for each component prediction using the value-quantile pairs provided by that model, and then obtains quasi-random samples from that distributional estimate. The number of samples drawn from the distribution of each component model defaults to `1e4`, but this can be changed using the `n_samples` argument.

In @fig-plot-ex-quantile-and-linear-pool, we compare ensemble results generated by `simple_ensemble` and `linear_pool` for model outputs of output types PMF and quantile. As expected, the results from the two functions are equivalent for the PMF output type: for this output type, the `simple_ensemble` method averages the predicted probability of each category across the component models, which is the definition of the linear pool ensemble method. This is not the case for the quantile output type, because the `simple_ensemble` is computing a quantile average.

```{r plot-ex-quantile-and-linear-pool}
#| label: fig-plot-ex-quantile-and-linear-pool
#| prompt: true
#| fig-cap: "Comparison of results from `simple_ensemble` (blue) and `linear
#| pool` (red). (Panel A) Ensemble predictions of US incident influenza
#| hospitalization intensity (classified as low, moderate, high, or very high),
#| which provide an example of PMF output type. (Panel B) Ensemble predictions
#| of weekly US incident influenza hospitalizations, which provide an example of
#| quantile output type. Note, for quantile output type, `simple_ensemble`
#| corresponds to a quantile average.  Ensembles combine individual models from
#| the example hub (@fig-plot-ex-mods)."
#| fig-width: 10
#| fig-height: 4

p1 <- dplyr::bind_rows(
  mean_ens,
  linear_pool_ens
) |>
  dplyr::filter(
    output_type == "pmf", reference_date == "2022-12-17",
    location == "US"
  ) |>
  dplyr::mutate(output_type_id = gsub("_", " ", output_type_id)) |>
  dplyr::mutate(output_type_id = factor(output_type_id,
    levels = c(
      "low", "moderate", "high",
      "very high"
    )
  )) |>
  ggplot(aes(x = output_type_id, y = value, fill = model_id)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(vars(horizon), labeller = label_both) +
  labs(x = "US incident hospitalization intensity", y = "probability") +
  scale_fill_brewer(palette = "Set1") +
  theme_bw() +
  theme(
    legend.position = "bottom", legend.title = element_blank(),
    strip.background = element_blank(), strip.placement = "outside",
    panel.grid.major.x = element_blank()
  )
model_output_plot <- linear_pool_ens |>
  dplyr::filter(output_type_id == 0.5) |>
  dplyr::mutate(output_type = "median", output_type_id = NA)
model_output_plot <- dplyr::bind_rows(linear_pool_ens, model_output_plot)
model_output_plot <- dplyr::bind_rows(mean_ens, model_output_plot) |>
  dplyr::filter(
    location == "US",
    output_type %in% c("median", "mean", "quantile"),
    reference_date == "2022-12-17"
  ) |>
  dplyr::mutate(target_date = reference_date + horizon)
target_data_plot <- hubEnsembles::example_target_data |>
  dplyr::filter(
    location == "US", time_idx >= "2022-11-01",
    time_idx <= "2023-03-01"
  )
p2 <-
  hubVis::plot_step_ahead_model_output(
    model_output_data = model_output_plot,
    truth_data = target_data_plot,
    use_median_as_point = TRUE,
    interactive = FALSE,
    intervals = c(0.5, 0.95),
    pal_color = "Set1",
    show_legend = TRUE,
    x_col_name = "target_end_date"
  ) +
  theme_bw() +
  labs(y = "US incident hospitalizations")
l <- cowplot::get_legend(p1)
cowplot::plot_grid(
  cowplot::plot_grid(
    p1 +
      labs(
        subtitle =
          "example PMF output type"
      ) +
      theme(legend.position = "none"),
    p2 +
      labs(
        subtitle =
          "example quantile output type"
      ) +
      theme(legend.position = "none"),
    labels = LETTERS[1:2]
  ), l,
  ncol = 1,
  rel_heights = c(0.95, 0.05)
)
```

## Case study: Weekly incident flu hospitalizations {#sec-flu}

To demonstrate the utility of the [hubEnsembles]{.pkg} package and the differences between the two ensembling functions, we examine a second set of weekly US influenza hospitalizations forecasts which were generated in real time.

Since 2013, the US Centers for Disease Control and Prevention (CDC) has been soliciting forecasts of seasonal influenza from modeling teams through a collaborative challenge called FluSight [@cdc_flusight]. We use a subset of these predictions to create four equally-weighted ensembles with `simple_ensemble()` and `linear_pool()` and compare the resulting ensembles' performance. The ensembling methods chosen for this case study consist of a quantile (arithmetic) mean, a quantile median, a linear pool with normal tails, and a linear pool with lognormal tails. Note that only a select portion of the code is shown in this manuscript for brevity, although all the functions and scripts used to generate the case study results can be found in the associated GitHub repository hubEnsemblesManuscript (<https://github.com/Infectious-Disease-Modeling-Hubs/hubEnsemblesManuscript>).

### Data and Methods
We begin by querying the component forecasts used to generate the four ensembles from zoltar, a repository designed to archive forecasts created by the Reich Lab at UMass Amherst. Here, we only consider Flusight predictions in a quantile format from the 2021-2022 and 2022-2023 seasons. These forecasts were stored in two data objects, split by season, called `flu_forecasts-raw_21-22.rds` and `flu_forecasts-raw_22-23.rds`. Since zoltar has its own formatting conventions, the raw forecasts must be transformed to fit hubverse standards before being fed into either of the ensembling functions. To do so, we use the `as_model_out_tbl()` function from the [hubUtils]{.pkg} package. Here, we specify the task ID variables `forecast date` (when the forecast was made), `location`, `horizon`, and `target`.


```{r transform data, eval=FALSE}
#| prompt: true

flu_forecasts_raw_21_22 <- readr::read_rds(
  here::here("analysis/data/raw_data/flu_forecasts-raw_21-22.rds")
)
flu_forecasts_raw_22_23 <- readr::read_rds(
  here::here("analysis/data/raw_data/flu_forecasts-raw_22-23.rds")
)
flu_forecasts_raw <- rbind(flu_forecasts_raw_21_22,
                           dplyr::select(flu_forecasts_raw_22_23, -season))

flu_forecasts_raw_21_22 <- readr::read_rds(
  here::here("analysis/data/raw_data/flu_forecasts-raw_21-22.rds")
)

flu_forecasts_hubverse <- flu_forecasts_raw |>
  dplyr::rename(target = target_long) |>
  as_model_out_tbl(
    model_id_col = "model_id",
    output_type_col = "output_type",
    output_type_id_col = "output_type_id",
    value_col = "value",
    sep = "-",
    trim_to_task_ids = FALSE,
    hub_con = NULL,
    task_id_cols = c(
      "forecast_date", "location", "horizon",
      "target"
    ),
    remove_empty = TRUE
  )
```

We excluded component forecasts from the ensemble calculations if any prediction (defined by a unique combination of task ID variables) did not include all 23 quantiles specified by FluSight ($\theta \in \{.010, 0.025, .050, .100, ..., .900, .950, .990\}$). We also excluded the FluSight baseline and median ensemble models that were previously generated. In practice, this means that all ensembles generated here include the real-time forecasts made by outside teams, matching the true fluctuation in the number of models used to generate the FluSight median ensemble (since number of submitted models varied from week to week).

With these inclusion criteria, the final data set of component forecasts consists of predictions from 25 modeling teams and 42 distinct models, 53 forecast dates (one per week), 54 US locations, 4 horizons, 1 target, and 23 quantiles. In the 2021-2022 season, 23 models made predictions for 22 weeks spanning from late January 2022 to late June 2022, and in the 2022-2023 season, there were 18 models making predictions for 31 weeks spanning mid-October 2022 to mid-May 2023. In both seasons, forecasts were made for the same locations (the 50 states, Washington DC, Puerto Rico, the Virgin Islands, and the US as a whole), horizons (1 to 4 weeks ahead), quantiles (the 23 described above), and target (week ahead incident flu hospitalization). The values for the forecasts are always non-negative. In @tbl-case-study-flu-forecasts, we provide an example of these predictions, showing select quantiles from a single model, forecast date, horizon, and location.

```{r, eval=TRUE}
#| label: tbl-case-study-flu-forecasts
#| prompt: true
#| tbl-cap: "An example prediction of weekly incident influenza
#| hospitalizations. This exmaple forecast was made on May 15, 2023 for
#| California at the 1 week ahead horizon. The forecast was generated during
#| theFluSight forecasting challenge, and formatted according to hubverse
#| standards post hoc. The `location`, `forecast_date`, and `season` columns
#| have been omitted for brevity."

readr::read_rds(
  here::here("analysis/data/raw_data/flu_forecasts-small.rds")
) |>
  dplyr::filter(
    model_id == "UMass-trends_ensemble",
    forecast_date == "2023-05-15",
    location == "06",
    horizon == 1,
    output_type_id %in% c(0.025, 0.1, 0.25, 0.75, 0.9, 0.975)
  ) |>
  dplyr::select(-location, -forecast_date, -season) |>
  knitr::kable()
```

Next, we combine the component model outputs using the following code to generate predictions from each ensemble model. The resulting ensemble forecasts will have the same task ID variables, model output specifications, and general data set features (albeit for 4 total models instead of 42).

```{r construct ensembles, eval=FALSE}
#| prompt: true

flu_forecasts_hubverse <- dplyr::filter(
  flu_forecasts_hubverse,
  model_id != "Flusight-baseline"
)
mean_ensemble <- hubEnsembles::simple_ensemble(flu_forecasts_hubverse,
  weights = NULL,
  agg_fun = "mean",
  model_id = "mean-ensemble"
)
median_ensemble <- hubEnsembles::simple_ensemble(flu_forecasts_hubverse,
  weights = NULL,
  agg_fun = "median",
  model_id = "median-ensemble"
)
lp_normal <- hubEnsembles::linear_pool(flu_forecasts_hubverse,
  weights = NULL,
  n_samples = 1e5, model_id = "lp-normal",
  tail_dist = "norm"
)
lp_lognormal <- hubEnsembles::linear_pool(flu_forecasts_hubverse,
  weights = NULL,
  n_samples = 1e5,
  model_id = "lp-lognormal",
  tail_dist = "lnorm"
)
```

After computing the various ensembles, we evaluate the performance using various scoring metrics. The goal of a forecast is to most accurately predict the future observation, and there are various metrics available to do evaluate a forecast with respect to this goal. Here, we use several common metrics in forecast evaluation, including mean absolute error (MAE), weighted interval score (WIS) [@bracher_evaluating_2021], 50% prediction interval (PI) coverage, and 95% PI coverage. Of these metrics, MAE evaluates how well a point forecast matches the corresponding observed value from the target data, and WIS and PI coverage evaluate how well a probabilistic forecast captures that same observed value, prioritizing probabilistic forecasts that are more certain. We briefly summarize each metric below.

```{r read in forecasts and scores, echo=FALSE}
devtools::load_all()
flu_truth_all <- readr::read_rds(
  here::here("analysis/data/raw_data/flu_truth_all.rds")
)
flu_files <- list.files(
  path = here::here("analysis/data/raw_data"),
  pattern = "hub",
  full.names = TRUE
)
flu_forecasts_ensembles <- purrr::map_dfr(flu_files, .f = readr::read_rds)
flu_scores_baseline <- readr::read_rds(
  here::here("analysis/data/raw_data/flu_baseline_scores.rds")
)
flu_scores_ensembles <- readr::read_rds(
  here::here("analysis/data/raw_data/flu_scores_ensembles.rds")
)
flu_scores_all <- rbind(flu_scores_ensembles, flu_scores_baseline)
```

MAE measures the average absolute error of a set of forecasts against the true value; smaller values of MAE indicate better forecast accuracy. WIS is a generalization of MAE for probabilistic forecasts and is an alternative to other common proper scoring rules which cannot be evaluated directly for quantile forecasts [@bracher_evaluating_2021]. WIS is made up of three component penalties: (1) for over-prediction, and (2) for under-prediction, which together measure the accuracy of the forecast, and (3) for the spread of each interval (where an interval is defined by a symmetric set of two quantiles). This metric weights these penalties across all prediction intervals provided. A lower WIS value indicates a more accurate forecast [@bracher_evaluating_2021]. Coverage is calculated for a particular prediction interval and provides information about whether a forecast has accurately characterized its uncertainty about future observations. The $(1-\alpha)*100$% PI coverage measures the proportion of the time that a given prediction interval at that nominal level includes the observed value. Achieving approximately nominal ($(1-\alpha)*100$%) coverage indicates a well-calibrated forecast. 

We also use relative versions of WIS and MAE (rWIS and rMAE, respectively) to understand how the ensemble performance compares to that of the FluSight baseline model. These metrics are calculated as $$\textrm{rWIS} = \frac{\textrm{WIS}_{\textrm{model }m}}{\textrm{WIS}_{\textrm{baseline}}} \hspace{3cm} \textrm{rMAE} = \frac{\textrm{MAE}_{\textrm{model }m}}{\textrm{MAE}_{\textrm{baseline}}},$$ where model $m$ is any given model being compared against the baseline. For both of these metrics, a value less than one indicates better performance compared to the baseline while a value greater than one indicates worse performance. By definition, the FluSight baseline itself will always have a value of one for both of these metrics.

Here, we score each unique prediction from an ensemble model, i.e., combination of task ID variables, against target data with the `score_forecasts()` function from the [covidHubUtils]{.pkg} package. This function outputs each of the metrics described above. We use median forecasts taken from the 0.5 quantile for the MAE evaluation. We calculate these metrics for the four ensembles (as well as the Flusight baseline) over all the forecasts using the `evaluate_flu_scores()` function stored in the `evaluation_functions.R` script. We run the ensemble and scoring code only one time and save the results in data objects split by a model and season, which then can be loaded separately for analysis plotting and scoring. 

### Performance results across ensembles
The quantile median ensemble had the best overall performance in terms of WIS and MAE (and the relative versions of these metrics) with above-nominal coverage rates (@tbl-overall-evaluation). The two linear opinion pools had very similar performance to each other. These methods had the second-best performance as measured by WIS and MAE, but they had the highest 50% and 95% coverage rates, with empirical coverage that was well above the nominal coverage rate. The quantile mean performed the worst of the ensembles with the highest MAE, which was substantially different from that of the other ensembles.

```{r overall-evaluation, message=FALSE, warning=FALSE, echo=FALSE}
#| label: tbl-overall-evaluation
#| tbl-cap: "Summary of overall model performance across both seasons, averaged
#| over all locations except the US national location. The best values for each
#| metric is bolded, though the metric values are often quite similar among
#| the models."
flu_overall_states <- flu_scores_all |>
  evaluate_flu_scores(
    grouping_variables = NULL,
    baseline_name = "Flusight-baseline", us_only = FALSE
  )
flu_overall_states |>
  dplyr::select(model, wis, rwis, mae, rmae, cov50, cov95) |>
  format_cells(rows = 1, cols = c(1:5, 7), "bold") |>
  format_cells(rows = 4, cols = 6, "bold") |>
  format_cells(rows = 1, cols = 6, "italics") |>
  knitr::kable()
```

Plots of the models' forecasts can aid our understanding about the origin of these accuracy differences. For example, the linear opinion pools consistently had some of the widest prediction intervals, and consequently the highest coverage rates. The median ensemble, which had the best WIS, balanced interval width with calibration best overall, with narrower intervals than the linear pools that still achieved near-nominal coverage on average across all time points. The quantile mean's interval widths could vary, though it usually had narrower intervals than the linear pools. However, this model's point forecasts demonstrated a larger error margin compared to the other ensembles, especially at longer horizons. This pattern is demonstrated in @fig-plot-forecasts-hubVis for the 4-week ahead forecast in California following the 2022-23 season peak on December 5, 2022. Here the quantile mean predicted a continued increase in hospitalizations, at a steeper slope than the other ensemble methods.

```{=html}
<!-- ELR: comments on the plot below:
 - can we get it so that there is not a line during the off season in this plot?
-->
```
```{r plot-forecasts, eval = FALSE, echo = FALSE}
#| label: fig-plot-forecasts
#| fig-cap: "One to four week ahead forecasts for select dates plotted against
#| target data for Vermont and California."

flu_forecasts_wide <- flu_forecasts_ensembles |>
  as_covid_hub_forecasts() |>
  dplyr::left_join(covidHubUtils::hub_locations_flusight,
                   by = c("location" = "fips")) |>
  dplyr::mutate(horizon = as.character(horizon))

select_dates <- all_flu_dates[seq(2, 53, 4)]

forecasts_vt <- flu_forecasts_wide |>
  dplyr::filter(
    location == "50",
    forecast_date %in% select_dates
  )

vt_plot <-
  covidHubUtils::plot_forecasts(
    forecasts_vt,
    hub = "FluSight",
    intervals = c(.50, .95),
    truth_data = dplyr::select(
      flu_truth_all, model,
      location,
      target_end_date,
      target_variable,
      value
    ),
    truth_source = "HealthData",
    use_median_as_point = TRUE,
    facet = model ~ .,
    facet_nrow = 5,
    fill_by_model = TRUE,
    fill_transparency = 0.75,
    # top_layer = c("forecasts", "truth"),
    title = "none",
    subtitle = "none",
    show_caption = FALSE,
    plot = FALSE
  )

max_truth_vt <- max(dplyr::filter(flu_truth_all, location == "50")$value)
max_forecast_vt <- max(dplyr::filter(forecasts_vt, location == "50")$value)

vt_plot <- vt_plot +
  scale_x_date(
    name = NULL, limits = c(
      as.Date("2022-01-01"),
      as.Date("2023-06-08")
    ),
    date_breaks = "4 months", date_labels = "%b '%y"
  ) +
  coord_cartesian(ylim = c(0, max(25, min(
    max_truth_vt * 3.5,
    max_forecast_vt
  )))) +
  ggtitle("Vermont") +
  theme(
    axis.ticks.length.x = unit(0.5, "cm"),
    axis.text.x = element_text(vjust = 7, hjust = -0.2),
    legend.position = "none"
  )

forecasts_ca <- flu_forecasts_wide |>
  dplyr::filter(
    location == "06",
    forecast_date %in% select_dates
  )

ca_plot <-
  covidHubUtils::plot_forecasts(forecasts_ca,
    hub = "FluSight",
    intervals = c(.50, .95),
    truth_data = dplyr::select(
      flu_truth_all, model,
      location,
      target_end_date,
      target_variable,
      value
    ),
    truth_source = "HealthData",
    use_median_as_point = TRUE,
    facet = model ~ .,
    facet_nrow = 5,
    fill_by_model = TRUE,
    #  fill_transparency = 0.75,
    #  top_layer = c("forecasts", "truth"),
    title = "none",
    subtitle = "none",
    show_caption = FALSE,
    plot = FALSE
  )

max_truth_ca <- max(filter(flu_truth_all, location == "06")$value)
max_forecast_ca <- max(filter(forecasts_ca, location == "06")$value)

ca_plot <- ca_plot +
  scale_x_date(
    name = NULL, limits = c(
      as.Date("2022-01-01"),
      as.Date("2023-06-08")
    ),
    date_breaks = "4 months", date_labels = "%b '%y"
  ) +
  coord_cartesian(ylim = c(0, max(25, min(
    max_truth_ca * 3.5,
    max_forecast_ca
  )))) +
  ggtitle("California") +
  theme(
    axis.ticks.length.x = unit(0.5, "cm"),
    axis.text.x = element_text(vjust = 7, hjust = -0.2),
    legend.position = "none"
  )

vt_plot + ca_plot +
  plot_layout(ncol = 2, guides = "collect") &
  plot_annotation(
    title = paste0(
      "Weekly Influenza Hospitalizations: observed",
      " and forecasted"
    ),
    subtitle = paste0(
      "Forecasts for Vermont and California, ",
      "every 4 weeks"
    )
  ) &
  theme(legend.position = "none")
```

```{r plot-forecasts-hubVis, warning=FALSE, message=FALSE}
#| label: fig-plot-forecasts-hubVis
#| prompt: true
#| fig-cap: "TO BE FIXED: One to four week ahead forecasts for select dates
#| plotted against target data for California. The first panel shows all models
#| on the same scale. All other panels show forecasts for each individual model,
#| with varying y-axis scales  show prediction accuracy as compared to
#| observed influenza hospitalizations."
model_names <- c(
  "Flusight-baseline", "lp-lognormal", "lp-normal",
  "mean-ensemble", "median-ensemble"
)

flu_dates_21_22 <- as.Date("2022-01-24") + weeks(0:21)
flu_dates_22_23 <- as.Date("2022-10-17") + weeks(0:30)
flu_dates_off_season <- as.Date("2022-06-27") + weeks(0:15)
all_flu_dates <- c(flu_dates_21_22, flu_dates_22_23)

select_dates <- c(all_flu_dates[seq(1, 69, 4)], flu_dates_21_22[22] +
                    lubridate::weeks(1:16))
forecasts_ca <- flu_forecasts_ensembles |>
  rbind(expand.grid(model_id = model_names[1:5],
                    forecast_date = flu_dates_21_22[22] +
                      lubridate::weeks(1:16),
                    location = unique(flu_truth_all$location),
                    horizon = 1:4,
                    target = "wk ahead inc flu hosp",
                    output_type = "quantile",
                    output_type_id = c(0.01, 0.025, seq(0.05, 0.95, 0.5),
                                       0.975, 0.99),
                    value = NA) |>
          dplyr::mutate(target_end_date = forecast_date +
                          lubridate::weeks(horizon), .before = target)) |>
  dplyr::filter(location == "06", forecast_date %in% select_dates) |>
  dplyr::group_by(forecast_date) |>
  as_model_out_tbl()

truth_ca <- flu_truth_all |>
  dplyr::filter(location == "06") |>
  rbind(expand.grid(
    model = "flu-truth",
    target_variable = "inc flu hosp",
    target_end_date = flu_dates_21_22[22] +
      lubridate::weeks(1:16),
    location = unique(flu_truth_all$location),
    value = NA
  ))

ca_plot_ensembles <-
  plot_step_ahead_model_output(
    forecasts_ca |> filter(model_id != "Flusight-baseline"),
    truth_ca,
    use_median_as_point = TRUE,
    show_plot = FALSE,
    x_col_name = "target_end_date",
    x_truth_col_name = "target_end_date",
    show_legend = FALSE,
    facet = "model_id",
    facet_scales = "free_y",
    facet_nrow = 3,
    interactive = FALSE,
    pal_color = "Set2",
    fill_transparency = 0.45,
    intervals = c(0.5, 0.95),
    title = NULL,
    group = "forecast_date"
  )

ca_plot_ensembles <- ca_plot_ensembles +
  scale_x_date(
    name = NULL, limits = c(
      as.Date("2022-01-01"),
      as.Date("2023-06-08")
    ),
    date_breaks = "4 months", date_labels = "%b '%y"
  ) +
  scale_color_manual(
    breaks = model_names[2:5],
    values = RColorBrewer::brewer.pal(5, "Set2")[2:5]
  ) +
  scale_fill_manual(
    breaks = model_names[2:5],
    values = RColorBrewer::brewer.pal(5, "Set2")[2:5]
  ) +
  theme(
    axis.ticks.length.x = unit(0.5, "cm"),
    axis.text.x = element_text(vjust = 7, hjust = -0.2),
    axis.title.y = element_blank(),
    legend.position = "none"
  )

ca_plot_baseline <-
  plot_step_ahead_model_output(
    forecasts_ca |> filter(model_id == "Flusight-baseline"),
    truth_ca,
    use_median_as_point = TRUE,
    show_plot = TRUE,
    x_col_name = "target_end_date",
    x_truth_col_name = "target_end_date",
    show_legend = FALSE,
    facet = "model_id",
    facet_scales = "free_y",
    facet_nrow = 1,
    interactive = FALSE,
    fill_transparency = 0.45,
    intervals = c(0.5, 0.95),
    title = NULL,
    group = "forecast_date"
  ) +
  scale_x_date(
    name = NULL, limits = c(
      as.Date("2022-01-01"),
      as.Date("2023-06-08")
    ),
    date_breaks = "4 months", date_labels = "%b '%y"
  ) +
  theme_bw() +
  theme(
    axis.title.x = element_blank(),
    axis.ticks.length.x = unit(0, "cm"),
    axis.text.x = element_blank(), # element_text(vjust = 5, hjust = -0.2),
    axis.title.y = element_blank(),
    legend.position = "none"
  )

ca_plot_all <-
  plot_step_ahead_model_output(
    forecasts_ca |> mutate(facet_name = "All models, same scale"),
    truth_ca,
    use_median_as_point = TRUE,
    show_plot = TRUE,
    x_col_name = "target_end_date",
    x_truth_col_name = "target_end_date",
    show_legend = FALSE,
    facet = "facet_name",
    interactive = FALSE,
    fill_transparency = 0.45,
    intervals = c(0.5, 0.95),
    title = NULL,
    group = "forecast_date"
  ) +
  scale_x_date(
    name = NULL, limits = c(
      as.Date("2022-01-01"),
      as.Date("2023-06-08")
    ),
    date_breaks = "4 months", date_labels = "%b '%y"
  ) +
  theme_bw() +
  theme(
    axis.title.x = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.length.x = unit(0, "cm"),
    axis.title.y = element_blank(),
    legend.position = "none"
  )

((ca_plot_all | ca_plot_baseline) / ca_plot_ensembles) +
  theme_bw() +
  plot_layout(guides = "collect", heights = c(1, 2)) +
  plot_annotation(title = paste0(
    "Weekly Incident ",
    "Hospitalizations for Influenza ",
    "in California"
  ))
```

We can use additional functions from the `evaluation_functions.R` script to examine model performance with greater granularity. For example, we may use the following lines of code to generate a table that shows scores from week to week.

```{r scores-vs-forecast-date, message=FALSE, warning=FALSE}
#| label: tab-scores-vs-forecast-date
#| prompt: true

flu_date_horizon_season_states <- flu_scores_all |>
  evaluate_flu_scores(
    grouping_variables = c(
      "horizon", "forecast_date",
      "season"
    ),
    baseline_name = "Flusight-baseline", us_only = FALSE
  )
```

However, reading and interpreting such a table becomes unwieldy with 53 weeks of forecasts. Instead, the above summarized scores can be separated by metric, filtered by horizon, and plotted against truth data, An example to generate such a plot using the `plot_evaluated_scores_forecast_date()` is shown in the code chunk below. (In practice, we may also split the predictions by season for improve readability, but this step is not strictly necessary.)

```{r metric-vs-forecast-date, eval=FALSE}
#| prompt: true
flu_date_horizon_season_states |>
  dplyr::filter(season == "2021-2022") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors,
    h = 1,
    y_var = "mae",
    main = "MAE 2021-2022, 1 week ahead",
    truth_data = flu_truth_all,
    truth_scaling = 0.1
  )
```

We then may combine several of these plots to obtain a complete picture by plotting every metric and the 1 and 4 week ahead horizons. From these combined plots we can see that the ensemble models tend to have similar MAE values during the entire time period, with slight divergence in MAE values for certain weeks at the four week ahead horizon (@fig-mae-vs-forecast-date). However, the models show greater differences for the other two metrics, WIS and coverage, particularly during times of rapid change in the observed incident hospitalizations (@fig-wis-vs-forecast-date and @fig-cov95-vs-forecast-date). The linear pools have a lower WIS than the median ensemble at the one week ahead forecast horizon for over a third of forecast dates (11 weeks) during the 2022-2023 season: from October 17, 2022 to December 12, 2022; January 2, 2023; and January 9, 2023 (@fig-wis-vs-forecast-date). These dates span the rapid rise and fall of incident flu hospitalizations surrounding the season's peak, with the largest differences in WIS occurring on November 28, December 5, December 12, January 2, and January 9. Additionally, the PI coverage rates for the linear pools were at least as large as the coverage rates of the other models throughout the entire period of analysis at both the 1 and 4 week ahead forecast horizons (see @fig-cov95-vs-forecast-date).

```{r mae-vs-forecast-date, message=FALSE, warning=FALSE, echo=FALSE}
#| label: fig-mae-vs-forecast-date
#| fig-cap: "Mean absolute error (MAE) averaged across all locations. Average
#| MAE is shown for each season (columns) and for 1-week and 4-week ahead
#| forecasts (rows). Results are plotted for each ensemble model (colored
#| points) across the entire season. Average target data across all locations
#| is plotted in black."
#| fig-width: 10
#| fig-height: 7.5
model_names <- c(model_names, "average target data")
# model_colors <- c("#6BAED6", "#FD8D3C", "#74C476", "#9E9AC8", "#FB6A4A")
model_colors <- c(RColorBrewer::brewer.pal(5, "Set2"), "black")

flu_date_horizon_season_states <- flu_scores_all |>
  evaluate_flu_scores(
    grouping_variables = c(
      "horizon", "forecast_date",
      "season"
    ),
    baseline_name = "Flusight-baseline", us_only = FALSE
  )

mae_date_plot_states1_2223 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2022-2023") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors,
    h = 1,
    y_var = "mae",
    main = "MAE 2022-2023, 1 week ahead",
    truth_data = flu_truth_all,
    truth_scaling = 0.15
  )
mae_date_plot_states4_2223 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2022-2023") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors,
    h = 4,
    y_var = "mae",
    main = "MAE 2022-2023, 4 week ahead",
    truth_data = flu_truth_all,
    truth_scaling = 0.15
  )

mae_date_plot_states1_2122 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2021-2022") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors,
    h = 1,
    y_var = "mae",
    main = "MAE 2021-2022, 1 week ahead",
    truth_data = flu_truth_all,
    truth_scaling = 0.15
  )
mae_date_plot_states4_2122 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2021-2022") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors,
    h = 4,
    y_var = "mae",
    main = "MAE 2021-2022, 4 week ahead",
    truth_data = flu_truth_all,
    truth_scaling = 0.15
  )

mae_date_plot_states1_2122 +
  mae_date_plot_states1_2223 +
  mae_date_plot_states4_2122 +
  mae_date_plot_states4_2223 +
  theme_bw() +
  plot_layout(ncol = 2, guides = "collect") &
  theme(legend.position = "bottom")
```

```{r wis-vs-forecast-date, message=FALSE, warning=FALSE, echo=FALSE}
#| label: fig-wis-vs-forecast-date
#| fig-cap: "Weighted interval score (WIS) averaged across all locations.
#| Average WIS is shown for each season (columns) and for 1-week and 4-week
#| ahead forecasts (rows). Results are plotted for each ensemble model (colored
#| points) across the entire season. Average target data across all locations is
#| plotted in black."
#| fig-width: 10
#| fig-height: 7.5

# WIS
wis_date_plot_states1_2122 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2021-2022") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors,
    h = 1,
    y_var = "wis",
    main = "WIS 2021-2022, 1 week ahead",
    truth_data = flu_truth_all,
    truth_scaling = 0.1
  )
wis_date_plot_states4_2122 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2021-2022") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors,
    h = 4,
    y_var = "wis",
    main = "WIS 2021-2022, 4 week ahead",
    truth_data = flu_truth_all,
    truth_scaling = 0.1
  )

wis_date_plot_states1_2223 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2022-2023") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors,
    h = 1,
    y_var = "wis",
    main = "WIS 2022-2023, 1 week ahead",
    truth_data = flu_truth_all,
    truth_scaling = 0.1
  )
wis_date_plot_states4_2223 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2022-2023") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors,
    h = 4,
    y_var = "wis",
    main = "WIS 2022-2023, 4 week ahead",
    truth_data = flu_truth_all,
    truth_scaling = 0.1
  )

wis_date_plot_states1_2122 +
  wis_date_plot_states1_2223 +
  wis_date_plot_states4_2122 +
  wis_date_plot_states4_2223 +
  theme_bw() +
  plot_layout(ncol = 2, guides = "collect") &
  theme(legend.position = "bottom")
```

```{r cov95-vs-forecast-date, message=FALSE, warning=FALSE, echo=FALSE}
#| label: fig-cov95-vs-forecast-date
#| fig-cap: "95% prediction interval (PI) coverage averaged across all
#| locations. Average coverage is shown for each season (columns) and for 1-week
#| and 4-week ahead forecasts (rows). Results are plotted for each ensemble
#| model (colored points) across the entire season. Average target data across
#| all locations is plotted in black."
#| fig-width: 9
#| fig-height: 7.5

# 95% Coverage
cov95_date_plot_states1_2122 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2021-2022") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors,
    h = 1,
    y_var = "cov95",
    main = "95% Coverage 2021-22, 1 week ahead",
    truth_data = flu_truth_all
  ) +
  coord_cartesian(ylim = c(0, 1.05))
cov95_date_plot_states4_2122 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2021-2022") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors,
    h = 4,
    y_var = "cov95",
    main = "95% Coverage 2021-22, 4 week ahead",
    truth_data = flu_truth_all
  ) +
  coord_cartesian(ylim = c(0, 1.05))

cov95_date_plot_states1_2223 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2022-2023") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors,
    h = 1,
    y_var = "cov95",
    main = "95% Coverage 2022-23, 1 week ahead",
    truth_data = flu_truth_all
  ) +
  coord_cartesian(ylim = c(0, 1.05))
cov95_date_plot_states4_2223 <- flu_date_horizon_season_states |>
  dplyr::filter(season == "2022-2023") |>
  plot_evaluated_scores_forecast_date(model_names, model_colors,
    h = 4,
    y_var = "cov95",
    main = "95% Coverage 2022-23, 4 week ahead",
    truth_data = flu_truth_all
  ) +
  coord_cartesian(ylim = c(0, 1.05))

truth_states_2122 <- flu_truth_all |>
  plot_flu_truth(
    date_range = flu_dates_21_22[c(1, 22)],
    main = "Target Data 2021-22 (wk inc flu hosp)"
  )
truth_states_2223 <- flu_truth_all |>
  plot_flu_truth(
    date_range = flu_dates_22_23[c(1, 31)],
    main = "Target Data 2022-23 (wk inc flu hosp)"
  )

truth_states_2122 + truth_states_2223 +
  cov95_date_plot_states1_2122 + cov95_date_plot_states1_2223 +
  cov95_date_plot_states4_2122 + cov95_date_plot_states4_2223 +
  theme_bw() +
  plot_layout(ncol = 2, guides = "collect", heights = c(1, 2, 2)) &
  theme(legend.position = "bottom")
```

In this analysis all of the ensemble variations outperformed the baseline model; yet, different ensembling methods performed best under different circumstances. While the quantile median had the best overall results for WIS, MAE, 50% PI coverage, and 95% PI coverage, other models may perform better from week-to-week for each metric. Around the 2022-2023 season's peak in early December, the remaining four models (including the baseline) each had instances in which they achieved the lowest WIS, like the linear pool ensembles for the one week ahead horizon over several weeks of this period.

The choice of an appropriate ensemble aggregation method may depend on the forecast target, the goal of forecasting, and the behavior of the individual models contributing to an ensemble. One case may call for prioritizing above-nominal coverage rates while another may prioritize accurate point forecasts. The `simple_ensemble` and `linear_pool` functions and the ability to specify component model weights and an aggregation function for `simple_ensemble` allow users to implement a variety of ensemble methods.

## Summary and discussion {#sec-conclusions}

Ensembles of independent models are a powerful tool to generate more accurate and more reliable forecasts of future outcomes than a single model alone. Here, we have demonstrated how to utilize [hubEnsembles]{.pkg}, a simple and flexible framework to combine individual model forecasts and create ensemble predictions. When using [hubEnsembles]{.pkg}, it is important to carefully choose an ensemble method that is well suited for the situation. Although there may not be a universal "best" method, matching the properties of a given ensemble method with the features of the component models will likely yield best results. For example, we showed for forecasts of seasonal influenza in the US, the quantile median ensemble performed best overall, but the linear pool method had advantages during periods of rapid change, when outlying component forecasts were likely more important. Notably, all ensemble methods outperformed the baseline model. These performance improvements from ensemble models motivate the use of a "hub-based" approach to prediction for infectious diseases and in other fields. Fitting within the larger suite of "hubverse" tools that support such efforts, the [hubEnsembles]{.pkg} package provides important software infrastructure for leveraging the power of multi-model ensembles.

## Acknowledgements  {.unnumbered}

The authors thank all members of the hubverse community; the broader hubverse software infrastructure made this package possible. E. Howerton was supported by the Eberly College of Science Barbara McClintock Science Achievement Graduate Scholarship in Biology at the Pennsylvania State University. PLEASE ADD FUNDING HERE.

Consortium of Infectious Disease Modeling Hubs authors include Alvaro J. Castro Rivadeneira (University of  Massachusetts Amherst), Lucie Contamin (University of Pittsburgh), Sebastian Funk (London School of Hygene and Tropical Medicine), Aaron Gerding (University of  Massachusetts Amherst), Hugo Gruson (ADD AFFILIATION), Harry Hochheiser (University of Pittsburgh), Emily Howerton (The Pennsylvania State University), Melissa Kerr (University of  Massachusetts Amherst), Anna Krystalli (R-RSE SMPC), Sara L. Loo (Johns Hopkins University), Evan L. Ray (University of Massachusetts Amherst), Nicholas G. Reich (University of Massachusetts Amherst), Koji Sato, Li Shandross (University of  Massachusetts Amherst), Katharine Sherratt (London School of Hygene and Tropical Medicine), Shaun Truelove (Johns Hopkins University, Martha Zorn (University of  Massachusetts Amherst)


## References {.unnumbered}
